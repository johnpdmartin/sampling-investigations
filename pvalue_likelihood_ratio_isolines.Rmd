---
title: Similarities and differences in behaviour of the BIC based  and likelihood ratio (LR) based Bayes Factor outputs under simple RCT analysis to two-sided p values from two sample t-tests for fixed sample sizes.
author: "John Martin"
date: "3/27/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Executive Summary

For simple Randomised Control Trial (RCT) analysis using a nested model approach the sample size normalised BIC based Bayes factor estimates is identical to a simple Likelihood Ratio (LR) based Bayes Factor behaviour 

$\frac{1}{\sqrt{N}}\frac{P_{BIC}(D|H0)}{P_{BIC}(D|H1)} = exp(-LR/2)\equiv \frac{\mathcal{L}(H0)}{\mathcal{L}(H1)}$.    



##Introduction

There is continuing disagreement [1,2] about the correlation between 

\begin{itemize}
\item $p_{value}$ the two sample t-test probability of the observed data given the Null Hypothesis and 
\item $P(H0|D)$ the Bayesian posterior probability of the Null Hypothesis given the observed data. 
\end{itemize}

In this paper, the normalised BIC based estimates Bayes Factor $\frac{1}{\sqrt{N}}\frac{P_{BIC}(D|H0)}{P_{BIC}(D|H1)}$ described in [3] which has a strong contour line relationship with $p_{value}$ for simple RCT analysis under repeated sampling for fixed sample size is illustrated to closely approach simple likelihood based Bayes Factor behaviour. 


Firstly, the contrasting behaviour for the relationship of BIC based Bayes Factor $P_{BIC}(D|H0)$ $\&$ likelihood based Bayes Factor $P_{LR}(D|H0)$ outputs with $p_{value}$ is shown for A/A $\&$ A/B conditions for RCT analysis under repeated sampling for fixed sample sizes. 

Then the close behaviour for the Bayes Factor estimates $\frac{1}{\sqrt{N}}\frac{P_{BIC}(D|H0)}{P_{BIC}(D|H1)}$  $\&$ $\frac{P_{LR}(D|H0)}{P_{LR}(D|H1)}$ is shown.

##BIC based Bayes Factor estimation

Masson [4] demonstrated how to calculate estimates of relative posterior probabilities using 

\begin{align} 
\frac{P(H0|D)}{P(H1|D)} &= \frac{P(D|H0))}{P(D|H1))} \cdot \frac{P(H0)}{P(H1)} \\
 &= BF \cdot \frac{P(H0)}{P(H1)}\label {eq:rel_post_prob}
\end{align}

where BF is the Bayes Factor of the ratio of the likelihoods of the data given H0 and H1 respectively, by using Bayesian Information Criteria (BIC) model fit calculations 

\begin{equation} 
BIC = -2ln(L) + k ln(n) \label {eq:BIC}
\end{equation}

where L is the maximum likelihood of the fitted model, k is the number of free model parameters and n is the sample size.

Explicitly, the BIC based estimate [4] for the Bayes Factor component in \eqref {eq:rel_post_prob}, is given by

\begin{align} 
\frac{P(H0|D)}{P(H1|D)} &\approx \frac{P_{BIC}(D|H0))}{P_{BIC}(D|H1))} \cdot \frac{P(H0)}{P(H1)} \\
&\approx e^{\frac{(\Delta BIC)}{2}} \cdot \frac{P(H0)}{P(H1)} \label {eq:app_rel_post_prob} \\
&\rightarrow e^{\frac{(\Delta BIC)}{2}} \qquad \qquad \qquad \qquad \qquad \text{as } \frac{P(H0)}{P(H1)} \rightarrow 1 \label {eq:app_prior_odds_1}
\end{align}


where 

\begin{equation} 
\Delta BIC = BIC_{H1}-BIC_{H0} \label {eq:dBIC}
\end{equation}

As illustrated in [3], a useful normalised version of the above estimator to compare to $p_{values}$ is

\begin{equation}
\text{norm}P_{BIC} = \frac{1}{\sqrt{N}}\frac{P_{BIC}(D|H0)}{P_{BIC}(D|H1)}
\end{equation}

##Likelihood Ratio based Bayes Factor estimation

The above approach by Masson can be replicated using a Likelihood Ratio (LR) based estimate for the Bayes Factor where for a nested set model

\begin{equation} 
LR = -2 \cdot ln(\frac{\mathcal{L}(H0)}{\mathcal{L}(H1)})  \label {eq:L}
\end{equation}

where $\mathcal{L}(H0)$, $\mathcal{L}(H1)$ are the maximum likelihoods of the fitted models.

Explicitly, the LR based estimate for the Bayes Factor component in \eqref {eq:rel_post_prob}, could be estimated by

\begin{align} 
\frac{P(H0|D)}{P(H1|D)} &\approx \frac{P_{LR}(D|H0))}{P_{LR}(D|H1))} \cdot \frac{P(H0)}{P(H1)} \\
&\approx e^{\frac{-(LR)}{2}} \cdot \frac{P(H0)}{P(H1)} \label {eq:appLR_rel_post_prob} \\
&\rightarrow e^{\frac{-(LR)}{2}} \qquad \qquad \qquad \qquad \qquad \text{as } \frac{P(H0)}{P(H1)} \rightarrow 1 \label {eq:appLR_prior_odds_1}
\end{align}


where 

\begin{align} 
e^{\frac{-(LR)}{2}} &= e^{-1 \cdot -2 \cdot ln(\frac{\mathcal{L}(H0)}{\mathcal{L}(H1)}) \cdot \frac{1}{2}} \\
&= e^{ln(\frac{\mathcal{L}(H0)}{\mathcal{L}(H1)}) } \\
&= \frac{\mathcal{L}(H0)}{\mathcal{L}(H1)} \label {eq:dLR}
\end{align}


Importantly, it is already demonstrated that the frequentist based null hypothesis significance test (NHST) method calculated via the probability of the observed t statistic under a two-sided null hypothesis can be related to a LR test [5,6].

Explicitly, the estimator for the numerator of the Likelihood based Bayes Factor component in \eqref {eq:rel_post_prob}, is given by

\begin{equation}
P_{LR}(D|H0)) = \frac{\frac{\mathcal{L}(H0)}{\mathcal{L}(H1)}}{\frac{\mathcal{L}(H0)}{\mathcal{L}(H1)}+1} \label {eq:PLRDH0}
\end{equation}

##Relationship between BIC based and Likelihood Ratio based Bayes Factors

Using equations \eqref {eq:BIC}, \eqref {eq:app_prior_odds_1} and  \eqref {eq:dLR} it can be seen for simple RCT analysis

\begin{align}
\frac{(\Delta BIC)}{2} &= \frac{(BIC_{H1}-BIC_{H0})}{2} \\
&= \frac{(-2ln(\mathcal{L}_{H1}) + 1 \cdot ln(n) + 2ln(\mathcal{L}_{H0}) )}{2} \\
&= \frac{(-2ln(\frac{\mathcal{L}_{H1}}{\mathcal{L}_{H0}}) + 1 \cdot ln(n)  )}{2} \\
&= \frac{(-2ln(\frac{\mathcal{L}_{H1}}{\mathcal{L}_{H0}}) - 2 \cdot ln(\frac{1}{n^{\frac{1}{2}}})  )}{2} \\
&= \frac{(-2ln(\frac{\mathcal{L}_{H1}}{\mathcal{L}_{H0}}\cdot \frac{1}{n^{\frac{1}{2}}}  )}{2} \\
&= -ln(\frac{\mathcal{L}_{H1}}{\mathcal{L}_{H0}}\cdot \frac{1}{n^{\frac{1}{2}}}  ) \\
\end{align}


\begin{align}
\therefore e^{\frac{(\Delta BIC)}{2}} &= e^{-ln(\frac{\mathcal{L}_{H1}}{\mathcal{L}_{H0}}\cdot \frac{1}{n^{\frac{1}{2}}}  )} \\
&= \frac{\sqrt{n}}{1} \cdot \frac{\mathcal{L}_{H0}}{\mathcal{L}_{H1}} \label {eq:} \\
&\equiv \sqrt{n} \cdot e^{-LR/2}
\end{align}

and hence for simple RCT analysis 

\begin{align}
\text{norm}P_{BIC} &= \frac{1}{\sqrt{n}}\frac{P_{BIC}(D|H0)}{P_{BIC}(D|H1)} \\
&\equiv \frac{\mathcal{L}(H0)}{\mathcal{L}(H1)} \label {eq:normBICIdentity}
\end{align}

which is bounded by (0,1) and has a known direct relationship to NHST $p_{values}$.


##Likelihood Ratio (and normalised BIC based) Bayes Factor vs $p_{value}$ behaviour for different sample sizes

As in [3] the following figure 1 shows the pseudo ROC curve relationship between $BF_{LR} \equiv \frac{BF_{BIC}}{\sqrt{N}}$ and $p_{value}$. On the figure are thresholds indicating strength of evidence for H1 including the p-value based significance regions, the Bayesian inference significance regions using Jeffreys [7] categories and the large N intersection of the p-value based and BIC based Bayesian thresholds (or simply the intersection of p-value and Likelihood Ratio based Bayes Factors) for simple RCT analysis.

An interpretation of the Likelihood Ratio based Bayes Factor is that it calculates the relative maximum likelihood performance of the null model (H0) compared to the alternative model (H1) in explaining the dependent data variable behaviour. Likewise using equations \eqref {eq:normBICIdentity} and \eqref {eq:BIC} the BIC based Bayes Factor calculates the relative maximum likelihood performance of the null model (H0) compared to a penalised alternative model (H1). With the conclusion that the penalty term for the alternative model (H1) is the factor that gives rise to Lindley's Paradox situations between BIC Bayes Factor estimates and NHST $p_{value}$ estimates (given there is a relative closeness between NHST $p_{value}$ cutoffs and Likelihood Ratio estimate significance cutoffs as echoed in figure 1.)   

```{r echo=FALSE, cache=TRUE, fig.height=7}

library(pracma)

elbow_finder <- function(x_values, y_values) {
  #https://stackoverflow.com/a/42810075
  # Max values to create line
  max_x_x <- max(x_values)
  max_x_y <- y_values[which.max(x_values)]
  max_y_y <- max(y_values)
  max_y_x <- x_values[which.max(y_values)]
  max_df <- data.frame(x = c(max_y_x, max_x_x), y = c(max_y_y, max_x_y))
  
  # Creating straight line between the max values
  fit <- lm(max_df$y ~ max_df$x)
  
  # Distance from point to line
  distances <- c()
  for(i in 1:length(x_values)) {
    distances <- c(distances, abs(coef(fit)[2]*x_values[i] - y_values[i] + coef(fit)[1]) / sqrt(coef(fit)[2]^2 + 1^2))
  }
  
  # Max distance point
  x_max_dist <- x_values[which.max(distances)]
  y_max_dist <- y_values[which.max(distances)]
  
  return(c(1-x_max_dist, y_max_dist))
}

par(mfrow=c(1,1))

data_calc=function(totsampsize,PH1_rate,effect_size,reps){
  
p_D_H0=0;p_D_H1=0;p_value=0;int_p_value=0;nsamp=totsampsize;nsamp2=nsamp/2;LR_D_H0=0;LR_D_H1=0
sd_samp=20;t_eff=effect_size/sqrt(2)*sqrt(10000/nsamp);effect=0.1*t_eff*sd_samp;PH1=PH1_rate;#t_eff=.5/sqrt(2)*sqrt(10000/nsamp);
for (i in 1:reps) {
  
  samp=runif(nsamp)#rnorm(nsamp,0,sd_samp)
  
  d=c(rep(1,nsamp2),rep(0,nsamp2))
  d2=c(rbinom(nsamp2,1,PH1),rep(0,nsamp2))
  
  signal=samp+effect*d2
  m1=lm(signal~d)
  m0=lm(signal~1)
  summary(m1)
  summary(m0)
  
  LR=exp((-2*(logLik(m1)-logLik(m0)))/2)
  BF=exp(((BIC(m1)-BIC(m0)))/2)
  
  LR_D_H0[i]=LR/(LR+1)
  LR_D_H1[i]=1-LR_D_H0[i]
  
  p_D_H0[i]=BF/(BF+1)
  p_D_H1[i]=1-p_D_H0[i]

    p_value[i]=summary(m1)$coefficients["d","Pr(>|t|)"]
  int_p_value[i]=summary(m0)$coefficients["(Intercept)","Pr(>|t|)"]
}

df=as.data.frame(cbind(p_value,p_D_H0,LR_D_H0))

}

tots=c(4,6,8,10,20,50,100,500,1000,10000,100000,1000000,
       10000000,100000000)
p_vals=c(.4990129,.3667658,.3140997,
         .2836638,.2241521,.1765258,.1498637,
         .103164,.0879358,.05036998,
         .02840425,.0157278,.008743237,0.004811645)
adj_pH0D=c(.7931323,.7764761,.7797581,
           .7839924,.8050357,.8332576,.8529681,
           .8930153,.9080369,.9458068,
           .9693143,.9828399,.9905736,0.9948048)

pH0D=c(.5287549,.5513773,.5760823,
       .5956358,.6579202,.7300176,.7754255,
       .854788,.8802024,.9364424,
       .9662587,.9818581,.9902605,0.9947053)


n10=data_calc(10,0,0,1000)
n10f=approxfun(n10$p_value,n10$p_D_H0)
n20=data_calc(20,0,0,1000)
n20f=approxfun(n20$p_value,n20$p_D_H0)
n50=data_calc(50,0,0,1000)
n50f=approxfun(n50$p_value,n50$p_D_H0)
n100=data_calc(100,0,0,1000)
n100f=approxfun(n100$p_value,n100$p_D_H0)
n1000=data_calc(1000,0,0,1000)
n1000f=approxfun(n1000$p_value,n1000$p_D_H0)
n500=data_calc(500,0,0,1000)
n500f=approxfun(n500$p_value,n500$p_D_H0)
n10000=data_calc(10000,0,0,1000)
n10000f=approxfun(n10000$p_value,n10000$p_D_H0)
n100000=data_calc(100000,0,0,1000)
n100000f=approxfun(n100000$p_value,n100000$p_D_H0)
n10g=approxfun(n10$p_value,n10$LR_D_H0)
n20g=approxfun(n20$p_value,n20$LR_D_H0)
n50g=approxfun(n50$p_value,n50$LR_D_H0)
n100g=approxfun(n100$p_value,n100$LR_D_H0)
n1000g=approxfun(n1000$p_value,n1000$LR_D_H0)
n500g=approxfun(n500$p_value,n500$LR_D_H0)
n10000g=approxfun(n10000$p_value,n10000$LR_D_H0)
n100000g=approxfun(n100000$p_value,n100000$LR_D_H0)




data_calcBF=function(totsampsize,PH1_rate,effect_size,reps){
  
  p_D_H0=0;p_D_H1=0;BFvec=0;p_value=0;int_p_value=0;nsamp=totsampsize;nsamp2=nsamp/2
  sd_samp=20;t_eff=effect_size/sqrt(2)*sqrt(10000/nsamp);effect=0.1*t_eff*sd_samp;PH1=PH1_rate;#t_eff=.5/sqrt(2)*sqrt(10000/nsamp);
  for (i in 1:reps) {
    
    samp=runif(nsamp)#rnorm(nsamp,0,sd_samp)
    
    d=c(rep(1,nsamp2),rep(0,nsamp2))
    d2=c(rbinom(nsamp2,1,PH1),rep(0,nsamp2))
    
    signal=samp+effect*d2
    m1=lm(signal~d)
    m0=lm(signal~1)
    summary(m1)
    summary(m0)
    
  LR=exp((-2*(logLik(m1)-logLik(m0)))/2)
  BF=exp(((BIC(m1)-BIC(m0)))/2)
  
    BFvec[i]=BF
    p_D_H0[i]=BF/(BF+1)
    p_D_H1[i]=1-p_D_H0[i]
    
    p_value[i]=summary(m1)$coefficients["d","Pr(>|t|)"]
    int_p_value[i]=summary(m0)$coefficients["(Intercept)","Pr(>|t|)"]
  }
  
  df=as.data.frame(cbind(p_value,BFvec))
  
}


#calculating with P(H0)=1 to start with
#to get universal contour lines first

n10=data_calcBF(10,0,0,1000)
n10BF=approxfun(n10$p_value,n10$BFvec)
n20=data_calcBF(20,0,0,1000)
n20BF=approxfun(n20$p_value,n20$BFvec)
n50=data_calcBF(50,0,0,1000)
n50BF=approxfun(n50$p_value,n50$BFvec)
n100=data_calcBF(100,0,0,1000)
n100BF=approxfun(n100$p_value,n100$BFvec)
n500=data_calcBF(500,0,0,1000)
n500BF=approxfun(n500$p_value,n500$BFvec)
n1000=data_calcBF(1000,0,0,1000)
n1000BF=approxfun(n1000$p_value,n1000$BFvec)
n10000=data_calcBF(10000,0,0,1000)
n10000BF=approxfun(n10000$p_value,n10000$BFvec)
n100000=data_calcBF(100000,0,0,1000)
n100000BF=approxfun(n100000$p_value,n100000$BFvec)

# recalculating for ~95% statistical power 
n10_95=data_calcBF(10,1,.0161,100)
n20_95=data_calcBF(20,1,.015,100)
n50_95=data_calcBF(50,1,.0149,100)
n100_95=data_calcBF(100,1,.0147,100)
n500_95=data_calcBF(500,1,.0143,100)
n1000_95=data_calcBF(1000,1,.0147,100)
n10000_95=data_calcBF(10000,1,.0147,100)
n100000_95=data_calcBF(100000,1,.0147,100)

plot(x=n100000_95$p_value,y=n100000_95$BFvec/sqrt(100000),xlim=c(0,1),col=rgb(0,0,0,0.5),ylab="BIC based (Bayes Factor)/sqrt(N)",pch=20,xlab="p_value",
     main=expression('(BIC based Bayes Factor)/sqrt(N) vs p_value'),col.main="blue",font.main=3,
     ylim=c(0,1))
#   ylim=c(.001,max(n100000$BFvec)*sqrt(10/100000)))
lines(x=seq(0,1,l=1001),n100000BF(seq(0,1,l=1001))/sqrt(100000),col=1)
abline(v=0.05,col=2,lty=2,lwd=2);
grid();
points(x=n10000_95$p_value,y=n10000_95$BFvec/sqrt(10000),col=2,pch=20)
lines(x=seq(0,1,l=1001),n10000BF(seq(0,1,l=1001))/sqrt(10000),col=2)
points(x=n1000_95$p_value,y=n1000_95$BFvec/sqrt(1000),col=3,pch=20)
lines(x=seq(0,1,l=1001),n1000BF(seq(0,1,l=1001))/sqrt(1000),col=3)
points(x=n500_95$p_value,y=n500_95$BFvec/sqrt(500),col=7,pch=20)
lines(x=seq(0,1,l=1001),n500BF(seq(0,1,l=1001))/sqrt(500),col=7)
points(x=n100_95$p_value,y=n100_95$BFvec/sqrt(100),col=4,pch=20)
lines(x=seq(0,1,l=1001),n100BF(seq(0,1,l=1001))/sqrt(100),col=4)
points(x=n50_95$p_value,y=n50_95$BFvec/sqrt(50),col=5,pch=20)
lines(x=seq(0,1,l=1001),n50BF(seq(0,1,l=1001))/sqrt(50),col=5)
points(x=n20_95$p_value,y=n20_95$BFvec/sqrt(20),col=6,pch=20)
lines(x=seq(0,1,l=1001),n20BF(seq(0,1,l=1001))/sqrt(20),col=6)
points(x=n10_95$p_value,y=n10_95$BFvec/sqrt(10),col="gray",pch=20)
lines(x=seq(0,1,l=1001),n10BF(seq(0,1,l=1001))/sqrt(10),col="gray")
abline(h=c(1/3,1/10,1/30,1/100,0),col=6,lty=6,lwd=2)
text(c(0.3,0.3,0.3,0.3,0.3,0.3,0.7),c(0.82,0.4,0.2,0.07,0.02,.01,0.155),labels=c("Jeffreys \n Bayesian Inference \n Regions \n giving evidence for H1","Bare Mention","Substantial","Strong","Very Strong","Decisive","large N max BIC based \n (Bayes Factor)/sqrt(N) ~ 0.15 \n using p-value < .05 constraint"),cex=0.8,srt=0)
text(c(0.01,.09),c(0.5,0.5),labels=c("p-value based regions \n Significant","Non-Significant"),cex=0.8,srt=90)
legend(x=.7,y=.9,legend=c("total N=100,000","total N=10,000",
                                "total N=1,000","total N=500","total N=100","total N=50","total N=20",
                                "total N=10","alpha = 0.05 p_value"),
       col=c(1,2,3,7,4,5,6,"gray",2),lty=c(rep(1,8),2),pch=c(rep(20,8),-1),cex=0.8)
abline(h=1,lty=3)#exp(-1/2))
abline(h=0.15,lty=3)
```
***Figure 1. ROC curve behaviour of LR based Bayes factor ($\equiv$ normalised BIC based (BF/sqrt(N))) is shown as a function of $p_{value}$ for different sample sizes. A overlay mapping of the Bayesian based strength of evidence regions (horizontal dot-dash lines) in favour of H1 and the $\alpha = 0.05$ based cutoffs to reject H0 (vertical dashed line) are also shown. The data points are from simple RCT analysis when $P(H1)=1$, the statistical power is ~95%, and for clarity only 100 repeated samples are shown. The contour lines behind the data points were obtained from figure 2 using $P(H0)=1$ to indicate the contour line positions are independent of the RCT treatment effect. ***





##Empirical behaviour of $P_{LR}(D|H0)$ & $P_{BIC}(D|H0)$ vs $p_{value}$ for simple RCT analysis under repeated sampling for different fixed sample sizes 

Figures 2 & 3, show the significant difference in $P_{LR}(D|H0)$ & $P_{BIC}(D|H0)$ outputs for simple RCT analysis. The $P_{LR}(D|H0)$ versus $p_{value}$ behaviour does not have the Lindley Paradox issues that is exhibited for $P_{BIC}(D|H0)$ versus $p_{value}$ behaviour.

It is informative that when $p_{value} = 1$, the numerator and denominator of the Likelihood Ratio based Bayes Factor are $P_{LR}(D|H0) = 0.5$ & $P_{LR}(D|H1) = 0.5$ indicating that both (unpenalised) models are equally efficient at explaining the data behaviour. Under these conditions the role of the prior odds becomes the determining factor in estimating the posterior odds.


###Under A/A conditions ie. $P(H1) = 0$ 


```{r echo=FALSE, cache=TRUE, fig.height=5.1}

n10=data_calc(10,0,0,1000)
n10f=approxfun(n10$p_value,n10$p_D_H0)
n20=data_calc(20,0,0,1000)
n20f=approxfun(n20$p_value,n20$p_D_H0)
n50=data_calc(50,0,0,1000)
n50f=approxfun(n50$p_value,n50$p_D_H0)
n100=data_calc(100,0,0,1000)
n100f=approxfun(n100$p_value,n100$p_D_H0)
n1000=data_calc(1000,0,0,1000)
n1000f=approxfun(n1000$p_value,n1000$p_D_H0)
n500=data_calc(500,0,0,1000)
n500f=approxfun(n500$p_value,n500$p_D_H0)
n10000=data_calc(10000,0,0,1000)
n10000f=approxfun(n10000$p_value,n10000$p_D_H0)
n100000=data_calc(100000,0,0,1000)
n100000f=approxfun(n100000$p_value,n100000$p_D_H0)
n10g=approxfun(n10$p_value,n10$LR_D_H0)
n20g=approxfun(n20$p_value,n20$LR_D_H0)
n50g=approxfun(n50$p_value,n50$LR_D_H0)
n100g=approxfun(n100$p_value,n100$LR_D_H0)
n1000g=approxfun(n1000$p_value,n1000$LR_D_H0)
n500g=approxfun(n500$p_value,n500$LR_D_H0)
n10000g=approxfun(n10000$p_value,n10000$LR_D_H0)
n100000g=approxfun(n100000$p_value,n100000$LR_D_H0)
  
par(mfrow=c(1,2))
plot(x=n100000$p_value,y=n100000$p_D_H0,xlim=c(0,1),col=rgb(0,0,0,0.5),
     ylab=expression('P'[BIC]*'(D | H0)'),
     ylim=c(0,1),pch=20,xlab="p_value",
     main=expression('P'[BIC]*'( D | H0 ) vs p_value for different total N in RCT analysis'),col.main="blue",font.main=3,cex.main=0.6)
lines(x=seq(0,1,l=1001),n100000f(seq(0,1,l=1001)),col=1)
abline(v=0.05,col=2,lty=2,lwd=2);
grid();
points(x=n10000$p_value,y=n10000$p_D_H0,col=2,pch=20)
lines(x=seq(0,1,l=1001),n10000f(seq(0,1,l=1001)),col=2)
points(x=n1000$p_value,y=n1000$p_D_H0,col=3,pch=20)
lines(x=seq(0,1,l=1001),n1000f(seq(0,1,l=1001)),col=3)
points(x=n500$p_value,y=n500$p_D_H0,col=7,pch=20)
lines(x=seq(0,1,l=1001),n500f(seq(0,1,l=1001)),col=7)
points(x=n100$p_value,y=n100$p_D_H0,col=4,pch=20)
lines(x=seq(0,1,l=1001),n100f(seq(0,1,l=1001)),col=4)
points(x=n50$p_value,y=n50$p_D_H0,col=5,pch=20)
lines(x=seq(0,1,l=1001),n50f(seq(0,1,l=1001)),col=5)
points(x=n20$p_value,y=n20$p_D_H0,col=6,pch=20)
lines(x=seq(0,1,l=1001),n20f(seq(0,1,l=1001)),col=6)
points(x=n10$p_value,y=n10$p_D_H0,col="gray",pch=20)
lines(x=seq(0,1,l=1001),n10f(seq(0,1,l=1001)),col="gray")
lines(y=pH0D,x=p_vals,col=2,lty=3,lwd=2)
out_elbow2=elbow_finder(c(1,1-n100000$p_D_H0),c(0,n100000$p_value));
legend(x="bottomright",legend=c("total N=100,000","total N=10,000",
"total N=1,000","total N=500","total N=100","total N=50","total N=20",
"total N=10","ROC cutoff points","alpha = 0.05 p_value"),
col=c(1,2,3,7,4,5,6,"gray",2,2),lty=c(rep(1,8),3,2),pch=c(rep(20,8),-1,-1),cex=0.6)


plot(x=n100000$p_value,y=n100000$LR_D_H0,xlim=c(0,1),col=rgb(0,0,0,0.5),
     ylab=expression('P'[LR]*'(D | H0)'),
     ylim=c(0,1),pch=20,xlab="p_value",
     main=expression('P'[LR]*'( D | H0 ) vs p_value for different total N in RCT analysis'),col.main="blue",font.main=3,cex.main=0.6)
lines(x=seq(0,1,l=1001),n100000g(seq(0,1,l=1001)),col=1)
abline(v=0.05,col=2,lty=2,lwd=2);
grid();
points(x=n10000$p_value,y=n10000$LR_D_H0,col=2,pch=20)
lines(x=seq(0,1,l=1001),n10000g(seq(0,1,l=1001)),col=2)
points(x=n1000$p_value,y=n1000$LR_D_H0,col=3,pch=20)
lines(x=seq(0,1,l=1001),n1000g(seq(0,1,l=1001)),col=3)
points(x=n500$p_value,y=n500$LR_D_H0,col=7,pch=20)
lines(x=seq(0,1,l=1001),n500g(seq(0,1,l=1001)),col=7)
points(x=n100$p_value,y=n100$LR_D_H0,col=4,pch=20)
lines(x=seq(0,1,l=1001),n100g(seq(0,1,l=1001)),col=4)
points(x=n50$p_value,y=n50$LR_D_H0,col=5,pch=20)
lines(x=seq(0,1,l=1001),n50g(seq(0,1,l=1001)),col=5)
points(x=n20$p_value,y=n20$LR_D_H0,col=6,pch=20)
lines(x=seq(0,1,l=1001),n20g(seq(0,1,l=1001)),col=6)
points(x=n10$p_value,y=n10$LR_D_H0,col="gray",pch=20)
lines(x=seq(0,1,l=1001),n10g(seq(0,1,l=1001)),col="gray")
abline(h=0.5,col=4,lty=3,lwd=2)
legend(x="topright",legend=c("total N=100,000","total N=10,000",
"total N=1,000","total N=500","total N=100","total N=50","total N=20",
"total N=10","alpha = 0.05 p_value","probability = 0.5"),
col=c(1,2,3,7,4,5,6,"gray",2,4),lty=c(rep(1,8),2,3),pch=c(rep(20,8),-1,-1),cex=0.6)



```
***Figure 2. $P_{BIC}(D|H0)$ as a function of p value for different sample sizes under simple RCT analysis when $P(H0)=1$.***

###Under A/B conditions ie. $P(H1) = 1$

```{r echo=FALSE, cache=TRUE, fig.height=5.1}


n10_2=data_calc(10,1,.01,1000)
n20_2=data_calc(20,1,.01,1000)
n50_2=data_calc(50,1,.01,1000)
n100_2=data_calc(100,1,.01,1000)
n1000_2=data_calc(1000,1,.01,1000)
n500_2=data_calc(500,1,.01,1000)
n10000_2=data_calc(10000,1,.01,1000)
n100000_2=data_calc(100000,1,.01,1000)

par(mfrow=c(1,2))
plot(x=n100000_2$p_value,y=n100000_2$p_D_H0,xlim=c(0,1),col=rgb(0,0,0,0.5),
     ylab=expression('P'[BIC]*'(D | H0)'),
     ylim=c(0,1),pch=20,xlab="p_value",
     main=expression('P'[BIC]*'( D | H0 ) vs p_value for different total N in RCT analysis'),col.main="blue",font.main=3,cex.main=0.6)
lines(x=seq(0,1,l=1001),n100000f(seq(0,1,l=1001)),col=1)
abline(v=0.05,col=2,lty=2,lwd=2);
grid();
points(x=n10000_2$p_value,y=n10000_2$p_D_H0,col=2,pch=20)
lines(x=seq(0,1,l=1001),n10000f(seq(0,1,l=1001)),col=2)
points(x=n1000_2$p_value,y=n1000_2$p_D_H0,col=3,pch=20)
lines(x=seq(0,1,l=1001),n1000f(seq(0,1,l=1001)),col=3)
points(x=n500_2$p_value,y=n500_2$p_D_H0,col=7,pch=20)
lines(x=seq(0,1,l=1001),n500f(seq(0,1,l=1001)),col=7)
points(x=n100_2$p_value,y=n100_2$p_D_H0,col=4,pch=20)
lines(x=seq(0,1,l=1001),n100f(seq(0,1,l=1001)),col=4)
points(x=n50_2$p_value,y=n50_2$p_D_H0,col=5,pch=20)
lines(x=seq(0,1,l=1001),n50f(seq(0,1,l=1001)),col=5)
points(x=n20_2$p_value,y=n20_2$p_D_H0,col=6,pch=20)
lines(x=seq(0,1,l=1001),n20f(seq(0,1,l=1001)),col=6)
points(x=n10_2$p_value,y=n10_2$p_D_H0,col="gray",pch=20)
lines(x=seq(0,1,l=1001),n10f(seq(0,1,l=1001)),col="gray")
lines(y=pH0D,x=p_vals,col=2,lty=3,lwd=2)
out_elbow2=elbow_finder(c(1,1-n100000_2$p_D_H0),c(0,n100000_2$p_value));
legend(x="bottomright",legend=c("total N=100,000","total N=10,000",
"total N=1,000","total N=500","total N=100","total N=50","total N=20",
"total N=10","ROC cutoff points","alpha = 0.05 p_value"),
col=c(1,2,3,7,4,5,6,"gray",2,2),lty=c(rep(1,8),3,2),pch=c(rep(20,8),-1,-1),cex=0.6)


plot(x=n100000_2$p_value,y=n100000_2$LR_D_H0,xlim=c(0,1),col=rgb(0,0,0,0.5),
     ylab=expression('P'[LR]*'(D | H0)'),
     ylim=c(0,1),pch=20,xlab="p_value",
     main=expression('P'[LR]*'( D | H0 ) vs p_value for different total N in RCT analysis'),col.main="blue",font.main=3,cex.main=0.6)
lines(x=seq(0,1,l=1001),n100000g(seq(0,1,l=1001)),col=1)
abline(v=0.05,col=2,lty=2,lwd=2);
grid();
points(x=n10000_2$p_value,y=n10000_2$LR_D_H0,col=2,pch=20)
lines(x=seq(0,1,l=1001),n10000g(seq(0,1,l=1001)),col=2)
points(x=n1000_2$p_value,y=n1000_2$LR_D_H0,col=3,pch=20)
lines(x=seq(0,1,l=1001),n1000g(seq(0,1,l=1001)),col=3)
points(x=n500_2$p_value,y=n500_2$LR_D_H0,col=7,pch=20)
lines(x=seq(0,1,l=1001),n500g(seq(0,1,l=1001)),col=7)
points(x=n100_2$p_value,y=n100_2$LR_D_H0,col=4,pch=20)
lines(x=seq(0,1,l=1001),n100g(seq(0,1,l=1001)),col=4)
points(x=n50_2$p_value,y=n50_2$LR_D_H0,col=5,pch=20)
lines(x=seq(0,1,l=1001),n50g(seq(0,1,l=1001)),col=5)
points(x=n20_2$p_value,y=n20_2$LR_D_H0,col=6,pch=20)
lines(x=seq(0,1,l=1001),n20g(seq(0,1,l=1001)),col=6)
points(x=n10_2$p_value,y=n10_2$LR_D_H0,col="gray",pch=20)
lines(x=seq(0,1,l=1001),n10g(seq(0,1,l=1001)),col="gray")
abline(h=0.5,col=4,lty=3,lwd=2)
legend(x="topright",legend=c("total N=100,000","total N=10,000",
"total N=1,000","total N=500","total N=100","total N=50","total N=20",
"total N=10","alpha = 0.05 p_value","probability = 0.5"),
col=c(1,2,3,7,4,5,6,"gray",2,4),lty=c(rep(1,8),2,3),pch=c(rep(20,8),-1,-1),cex=0.6)


```
***Figure 3. $P_{BIC}(D|H0)$ as a function of p value for different sample sizes under simple RCT analysis when $P(H1)=1$. As evidence, of fixed contour line behaviour the fitted lines on the graph are from $P(H1)=0$ data and these lines fit the $P(H1)=1$ data points very well. ***




##Conclusions

By normalising the BIC based Bayes Factor estimates for simple RCT analysis, the values are identical to a Likelihood Ratio based Bayes Factor approach and the Lindley Paradox conflict is not relevant.

##References

1. Trafimow, D., & Rice, S. (2009). A test of the null hypothesis significance testing procedure correlation argument. The Journal of General Psychology, 136, 261-270.

2. Lakens, D. (2015) https://daniellakens.blogspot.com/2015/11/the-relation-between-p-values-and.html

3. Martin, J.P.D. (2020) The strong contour line relationship between two-sided p values of simple RCT two sample t-tests and BIC based Bayes factor $P_{BIC}(D|H0)$ outputs for fixed sample sizes.
DOI 10.6084/m9.figshare.12003945

4. Masson, M. E. J. (2011) A tutorial on a practical Bayesian alternative to null-hypothesis significance testing. Behav Res 43:679–690  DOI 10.3758/s13428-010-0049-5

5. Neyman, J., Pearson, E. S. (1933) On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London A, 231 (694–706) doi:10.1098/rsta.1933.0009, 

6. Francis, G. (2017) Equivalent statistics and data interpretation. Behav Res 49:1524–1538 https://doi.org/10.3758/s13428-016-0812-3

7. Jeffreys H. (1961) Theory of Probability. Oxford, UK: Oxford Univ. Press. 3rd ed.

