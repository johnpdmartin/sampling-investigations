plot(x=reg4_100perc[,2],y=reg4_100perc[,28],main=paste("df adj in perc scale","\n n=100"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 8b",cex.main=0.9)
lines(x=reg4_100perc[,2],y=reg4_100perc[,29],col="blue",lwd=2)
lines(x=reg4_100perc[,2],y=reg4_100perc[,30],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.666,1,0.45,0.9), new=TRUE)
plot(x=reg4_1000perc[,2],y=reg4_1000perc[,28],main=paste("df adj in perc scale","\n n=1000"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 8c",cex.main=0.9)
lines(x=reg4_1000perc[,2],y=reg4_1000perc[,29],col="blue",lwd=2)
lines(x=reg4_1000perc[,2],y=reg4_1000perc[,30],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.0,.333,0.0,0.45), new=TRUE)
plot(x=reg4_50org[,2],y=reg4_50org[,28],main=paste("df adj in org scale","\n n=50"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 8d",cex.main=0.9)
lines(x=reg4_50org[,2],y=reg4_50org[,29],col="blue",lwd=2)
lines(x=reg4_50org[,2],y=reg4_50org[,30],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.333,.666,0.0,0.45), new=TRUE)
plot(x=reg4_100org[,2],y=reg4_100org[,28],main=paste("df adj in org scale","\n n=100"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 8e",cex.main=0.9)
lines(x=reg4_100org[,2],y=reg4_100org[,29],col="blue",lwd=2)
lines(x=reg4_100org[,2],y=reg4_100org[,30],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.666,1,0.0,0.45), new=TRUE)
plot(x=reg4_1000org[,2],y=reg4_1000org[,28],main=paste("df adj in org scale","\n n=1000"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 8f",cex.main=0.9)
lines(x=reg4_1000org[,2],y=reg4_1000org[,29],col="blue",lwd=2)
lines(x=reg4_1000org[,2],y=reg4_1000org[,30],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
mtext("model (iv): quadratic term slope CI coverage", side=3, outer=TRUE, line=-1.5)
mtext("black - bootstrap, blue line - evd_max, red lines - bin_max", side=3, outer=TRUE, line=-3)
```
```{r, reg4intercept, echo=FALSE, cache=FALSE, fig.width=6, fig.height=6, fig.keep='high', message=FALSE, warning=FALSE}
par(mfrow=c(2,3))
par(fig=c(0,.333,0.45,0.9))
plot(x=reg4_50perc[,2],y=reg4_50perc[,15],main=paste("df adj in perc scale","\n n=50"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 9a",cex.main=0.9)
lines(x=reg4_50perc[,2],y=reg4_50perc[,16],col="blue",lwd=2)
lines(x=reg4_50perc[,2],y=reg4_50perc[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.333,.666,0.45,0.9), new=TRUE)
plot(x=reg4_100perc[,2],y=reg4_100perc[,15],main=paste("df adj in perc scale","\n n=100"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 9b",cex.main=0.9)
lines(x=reg4_100perc[,2],y=reg4_100perc[,16],col="blue",lwd=2)
lines(x=reg4_100perc[,2],y=reg4_100perc[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.666,1,0.45,0.9), new=TRUE)
plot(x=reg4_1000perc[,2],y=reg4_1000perc[,15],main=paste("df adj in perc scale","\n n=1000"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 9c",cex.main=0.9)
lines(x=reg4_1000perc[,2],y=reg4_1000perc[,16],col="blue",lwd=2)
lines(x=reg4_1000perc[,2],y=reg4_1000perc[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.0,.333,0.0,0.45), new=TRUE)
plot(x=reg4_50org[,2],y=reg4_50org[,15],main=paste("df adj in org scale","\n n=50"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 9d",cex.main=0.9)
lines(x=reg4_50org[,2],y=reg4_50org[,16],col="blue",lwd=2)
lines(x=reg4_50org[,2],y=reg4_50org[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.333,.666,0.0,0.45), new=TRUE)
plot(x=reg4_100org[,2],y=reg4_100org[,15],main=paste("df adj in org scale","\n n=100"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 9e",cex.main=0.9)
lines(x=reg4_100org[,2],y=reg4_100org[,16],col="blue",lwd=2)
lines(x=reg4_100org[,2],y=reg4_100org[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.666,1,0.0,0.45), new=TRUE)
plot(x=reg4_1000org[,2],y=reg4_1000org[,15],main=paste("df adj in org scale","\n n=1000"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 9f",cex.main=0.9)
lines(x=reg4_1000org[,2],y=reg4_1000org[,16],col="blue",lwd=2)
lines(x=reg4_1000org[,2],y=reg4_1000org[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
mtext("model (iv): intercept CI coverage", side=3, outer=TRUE, line=-1.5)
mtext("black - bootstrap, blue line - evd_max, red lines - bin_max", side=3, outer=TRUE, line=-3)
```
```{r, reg5slope, echo=FALSE, cache=FALSE, fig.width=6, fig.height=6, fig.keep='high', message=FALSE, warning=FALSE}
setwd("C:/Users/John/Documents/GitHub/sampling-investigations/modelCIhomogeneouscaseresults")
# library("quantreg")
reg5_50perc <- read.csv(file="../modelCIhomogeneouscaseresults/50_noniid_gaussian_10_dfadj_in_perc_scale.csv")
reg5_100perc <- read.csv(file="../modelCIhomogeneouscaseresults/100_noniid_gaussian_10_dfadj_in_perc_scale.csv")
reg5_1000perc <- read.csv(file="../modelCIhomogeneouscaseresults/1000_noniid_gaussian_10_dfadj_in_perc_scale.csv")
reg5_50org <- read.csv(file="../modelCIhomogeneouscaseresults/50_noniid_gaussian_10_dfadj_in_org_scale.csv")
reg5_100org <- read.csv(file="../modelCIhomogeneouscaseresults/100_noniid_gaussian_10_dfadj_in_org_scale.csv")
reg5_1000org <- read.csv(file="../modelCIhomogeneouscaseresults/1000_noniid_gaussian_10_dfadj_in_org_scale.csv")
par(mfrow=c(2,3))
par(fig=c(0,.333,0.45,0.9))
plot(x=reg5_50perc[,2],y=reg5_50perc[,5],main=paste("df adj in perc scale","\n n=50"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 10a",cex.main=0.9)
lines(x=reg5_50perc[,2],y=reg5_50perc[,6],col="blue",lwd=2)
lines(x=reg5_50perc[,2],y=reg5_50perc[,7],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.333,.666,0.45,0.9), new=TRUE)
plot(x=reg5_100perc[,2],y=reg5_100perc[,5],main=paste("df adj in perc scale","\n n=100"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 10b",cex.main=0.9)
lines(x=reg5_100perc[,2],y=reg5_100perc[,6],col="blue",lwd=2)
lines(x=reg5_100perc[,2],y=reg5_100perc[,7],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.666,1,0.45,0.9), new=TRUE)
plot(x=reg5_1000perc[,2],y=reg5_1000perc[,5],main=paste("df adj in perc scale","\n n=1000"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 10c",cex.main=0.9)
lines(x=reg5_1000perc[,2],y=reg5_1000perc[,6],col="blue",lwd=2)
lines(x=reg5_1000perc[,2],y=reg5_1000perc[,7],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.0,.333,0.0,0.45), new=TRUE)
plot(x=reg5_50org[,2],y=reg5_50org[,5],main=paste("df adj in org scale","\n n=50"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 10d",cex.main=0.9)
lines(x=reg5_50org[,2],y=reg5_50org[,6],col="blue",lwd=2)
lines(x=reg5_50org[,2],y=reg5_50org[,7],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.333,.666,0.0,0.45), new=TRUE)
plot(x=reg5_100org[,2],y=reg5_100org[,5],main=paste("df adj in org scale","\n n=100"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 10e",cex.main=0.9)
lines(x=reg5_100org[,2],y=reg5_100org[,6],col="blue",lwd=2)
lines(x=reg5_100org[,2],y=reg5_100org[,7],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.666,1,0.0,0.45), new=TRUE)
plot(x=reg5_1000org[,2],y=reg5_1000org[,5],main=paste("df adj in org scale","\n n=1000"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 10f",cex.main=0.9)
lines(x=reg5_1000org[,2],y=reg5_1000org[,6],col="blue",lwd=2)
lines(x=reg5_1000org[,2],y=reg5_1000org[,7],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
mtext("model (v): slope CI coverage", side=3, outer=TRUE, line=-1.5)
mtext("black - bootstrap, blue line - evd_max, red lines - bin_max", side=3, outer=TRUE, line=-3)
```
```{r, reg5intercept, echo=FALSE, cache=FALSE, fig.width=6, fig.height=6, fig.keep='high', message=FALSE, warning=FALSE}
par(mfrow=c(2,3))
par(fig=c(0,.333,0.45,0.9))
plot(x=reg5_50perc[,2],y=reg5_50perc[,15],main=paste("df adj in perc scale","\n n=50"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 11a",cex.main=0.9)
lines(x=reg5_50perc[,2],y=reg5_50perc[,16],col="blue",lwd=2)
lines(x=reg5_50perc[,2],y=reg5_50perc[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.333,.666,0.45,0.9), new=TRUE)
plot(x=reg5_100perc[,2],y=reg5_100perc[,15],main=paste("df adj in perc scale","\n n=100"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 11b",cex.main=0.9)
lines(x=reg5_100perc[,2],y=reg5_100perc[,16],col="blue",lwd=2)
lines(x=reg5_100perc[,2],y=reg5_100perc[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.666,1,0.45,0.9), new=TRUE)
plot(x=reg5_1000perc[,2],y=reg5_1000perc[,15],main=paste("df adj in perc scale","\n n=1000"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 11c",cex.main=0.9)
lines(x=reg5_1000perc[,2],y=reg5_1000perc[,16],col="blue",lwd=2)
lines(x=reg5_1000perc[,2],y=reg5_1000perc[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.0,.333,0.0,0.45), new=TRUE)
plot(x=reg5_50org[,2],y=reg5_50org[,15],main=paste("df adj in org scale","\n n=50"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 11d",cex.main=0.9)
lines(x=reg5_50org[,2],y=reg5_50org[,16],col="blue",lwd=2)
lines(x=reg5_50org[,2],y=reg5_50org[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.333,.666,0.0,0.45), new=TRUE)
plot(x=reg5_100org[,2],y=reg5_100org[,15],main=paste("df adj in org scale","\n n=100"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 11e",cex.main=0.9)
lines(x=reg5_100org[,2],y=reg5_100org[,16],col="blue",lwd=2)
lines(x=reg5_100org[,2],y=reg5_100org[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
par(fig=c(.666,1,0.0,0.45), new=TRUE)
plot(x=reg5_1000org[,2],y=reg5_1000org[,15],main=paste("df adj in org scale","\n n=1000"),ylim=c(0.75,1),typ="b",xlab="quantile",ylab="coverage",sub="Fig. 11f",cex.main=0.9)
lines(x=reg5_1000org[,2],y=reg5_1000org[,16],col="blue",lwd=2)
lines(x=reg5_1000org[,2],y=reg5_1000org[,17],col="red",lwd=2)
abline(h=0.95,col="green",lwd=2,lty=2)
mtext("model (v): intercept CI coverage", side=3, outer=TRUE, line=-1.5)
mtext("black - bootstrap, blue line - evd_max, red lines - bin_max", side=3, outer=TRUE, line=-3)
```
#Comparing coverage performance for a small sample "real world" example
To round off the CI performance comparison, appendix A, contains a comparison of the evd based model coefficient CI estimator (evd_symm_max using degrees of freedom adjustment in percentile scale) to the quantreg default bootstrap and rank inversion CI estimates for the Engel food expenditure dataset [8]. Providing this example also allows, the r markdown version of this paper [9], to conveniently contain a concise version of the evd based model coefficient CI estimator algorithm rather than the lengthy repeated sampling version.
This Engel dataset when analysed in log-log form provides an interesting example of near homoscedastic iid performance to underline the difference in performance expected for CI estimators between "ideal" error distributions and real world examples.
# Conclusions
For homoscedastic iid unweighted cases, both the evd_max and bin_max evd based approximations to quantile regression model coefficient CIs show good performance for model slope CIs coverage for moderate samples n=1000. For smaller samples, the coverage performance is weaker for extreme quantiles.
For model intercept CI estimates, the evd_max and bin_max evd based approximations have good coverage for linear cases but slight undercoverage (93%) for nonlinear examples, with the same sample size dependence as for model slope CI estimates.
It would be worthwhile investigating, (i) minor improvements to equation \eqref{eq:5} for model intercept CI estimates and (ii) heteroscedastic consistent standard error extensions of the current approach.
#References
1. Martin J.P.D., 2015, http://dx.doi.org/10.6084/m9.figshare.1566828
2. Martin J.P.D., 2015, http://dx.doi.org/10.6084/m9.figshare.1591019
3. Koencker, R. W. & Bassett G., Econometrica, 1978, vol. 46, issue 1, pages 33-50
4. Koencker, R. W., Portnoy S. et al, https://cran.r-project.org/web/packages/quantreg/quantreg.pdf
5. https://en.wikipedia.org/wiki/Quantile
6. Brown, B. M. and Wang, Y.-G. (2005). Standard errors and covariance
matrices for smoothed rank estimators. Biometrika 92 149-158. MR2158616
7. Agresti, A. and Coull, B. A. 1998 The American Statistician, vol. 52, p119-126. doi:10.2307/2685469. JSTOR 2685469
8. https://cran.r-project.org/web/packages/quantreg/vignettes/rq.pdf
9. https://github.com/johnpdmartin/sampling-investigations/blob/master/quantile_regression_model_coefficient_CIs_using_the_empirical_variance_distribution_approximation.Rmd
# Appendix A: Comparison of model coefficient CIs for Engel dataset
The Engel dataset is a small sample example of the heteroscedastic relationship between food expenditure and household income and is included in the quantreg package as an quantile regression example.
In log linear form, by taking the logarithms of both food expenditure and household income prior to quantile regression analysis, the error distribution is almost homoscedastic in nature, as shown in Figure A1.
```{r, engel_log-log, echo=FALSE, cache=FALSE, fig.width=6, fig.height=6, fig.keep='high',warning=FALSE,message=FALSE}
library(quantreg)
#example(rq)
data(engel)
par(mfrow=c(1,1))
par(mar=c(5,6,4,3)+0.1)
plot(engel$income,engel$foodexp,log="xy",xlab="Household Income",
ylab="Food Expenditure")
taus <- c(.05,.1,.25,.75,.90,.95)
abline(rq(log10(engel$foodexp)~log10(engel$income),tau=.5),col="blue")
abline(lm(log10(engel$foodexp)~log10(engel$income)),lty = 3,col="red")
for( i in 1:length(taus)){
abline(rq(log10(engel$foodexp)~log10(engel$income),tau=taus[i]),col="gray")
}
mtext("Quantile regression of Engel dataset using log-log transformation", side=3, outer=TRUE, line=-1.5)
```
***Figure A1: Engel dataset in log-linear form***
Figure A2, shows the model coefficents CI estimates of the intercept and slope, using
1. rank inversion
2. quantreg package default bootstrap (delete-d-group jackknife)
3. evd_symm_max with the degrees of freedom calculated in the percentile scale
where the outline of the bootstrap CIs is given in all the sub-graphs.
The rank inversion method was replaced as the quantreg default quantile regression CI estimator by the bootstrap method. As seen in figure A2, for this quasi-iid homoscedastic case, the bootstrap and evd_symm_max estimates are similar for many quantiles except for minor differences for regions near 0.4-0.5 and 0.6-0.8. The difference occur in both the slope and intercept CI estimates suggesting a breakdown of the iid conditions for the evd_symm_max method. The older rank inversion method exhibits smaller CIs compared to the other two methods for quasi-iid homoscedastic conditions.
```{r, evd_algorithm, echo=FALSE, cache=FALSE, fig.width=6, fig.height=6, fig.keep='high',warning=FALSE}
library(quantreg)
#example(rq)
data(engel)
CI_estimator <- function(y_data,x_data,dfadj_scale) {
n_samp <- length(x_data)
method_df <- dfadj_scale # "perc_scale" # "org_scale" #
t_adj <- abs(qt(.025,n_samp))
wgt_filereg <- rep(1,n_samp)
taus_set <- seq(.02,.98,length.out=50)
x_sample <- x_data
y_sample <- y_data
med_est_x <- rq((x_sample)~(1),tau=.5,weights=wgt_filereg)
med_pt_est_x <- med_est_x$coefficients[1]
boot_low <- 0;boot_high <- 0
rank_low <- 0;rank_high <- 0
evd_low <- 0;evd_high <- 0
evd_symm_low <- 0;evd_symm_high <- 0
bin_low <- 0 ;bin_high <- 0
boot_y0_low <- 0;boot_y0_high <- 0
rank_y0_low <- 0;rank_y0_high <- 0
evd_y0_low <- 0;evd_y0_high <- 0
evd_symm_y0_low <- 0;evd_symm_y0_high <- 0
bin_y0_low <- 0 ;bin_y0_high <- 0
pt_est_b0 <- 0;pt_est_b1 <- 0
for (i in 1:length(taus_set)) {
#taus <- .05
taus <- taus_set[i]
if(method_df == "perc_scale") {
q_var <- sqrt(taus*(1-taus)/(n_samp-2))} else
{q_var <- sqrt(taus*(1-taus)/(n_samp))}
p_low <- qnorm(0.025,taus,q_var)
p_high <- qnorm(0.975,taus,q_var)
p_low <- max(p_low,0.0001)
p_high <- min(p_high,0.9999)
if(method_df == "perc_scale")  {
b_low <- qbinom(.025,n_samp-2,taus)/(n_samp-2)
b_high <- qbinom(.975,n_samp-2,taus)/(n_samp-2)}
else  {b_low <- qbinom(.025,n_samp,taus)/(n_samp)
b_high <- qbinom(.975,n_samp,taus)/(n_samp)}
#   print("binomial CI | evd CI  , in percentile scale")
#   print("lower bounds")
#   print(paste(b_low,p_low))
#   print("upper bounds")
#   print(paste(b_high,p_high))
model_fit_rq <- rq((y_sample)~(x_sample),tau=taus,weights=wgt_filereg)
model_fit_rq
rqr_beta0 <- model_fit_rq$coefficients[1]
rqr_beta1 <- model_fit_rq$coefficients[2]
std_errors <- summary(model_fit_rq,se="boot",R=600)$coefficients[c(3:4)]
std_errorsrank <- summary(model_fit_rq,se="rank")$coefficients[c(3:6)]
beta1_std <- std_errors[2]
beta0_std <- std_errors[1]
beta1_rlb <- std_errorsrank[2]
beta0_rlb <- std_errorsrank[1]
beta1_rub <- std_errorsrank[4]
beta0_rub <- std_errorsrank[3]
y_var <- var(y_sample)
samp <- y_sample-rqr_beta0-rqr_beta1*x_sample
samp_var <- var(samp)
rho <- function(u,taur=.5) u*(taur - (u < 0))
samp_r1 <- 1 - model_fit_rq$rho/rq(y_sample~1,tau=taus,weights=wgt_filereg)$rho
model_fit <- rq((samp)~(1),tau=taus,method="fn",weights=wgt_filereg)
qr_dat <- c(summary(model_fit)$coefficients[1])
qrboot_dat  <- c(summary(model_fit,se="boot",R=600)$coefficients[1:4])
back_pts1 <- rq((samp)~(1),tau=p_low,weights=wgt_filereg)$coefficients[1]
back_pts2 <- rq((samp)~(1),tau=p_high,weights=wgt_filereg)$coefficients[1]
back_pts3 <- rq((samp)~(1),tau=b_low,weights=wgt_filereg)$coefficients[1]
back_pts4 <- rq((samp)~(1),tau=b_high,weights=wgt_filereg)$coefficients[1]
bk_pt <- c(back_pts1,back_pts2,back_pts3,back_pts4)
# print(bk_pt)
#   s2x1x2 <- sum(x_sample*x_sample2)-sum(x_sample)*sum(x_sample2)/n_samp
s2x1 <- sum(x_sample^2)-sum(x_sample)^2/n_samp
#   s2x2 <- sum(x_sample2^2)-sum(x_sample2)^2/n_samp
#    r12 <- s2x1x2/sqrt(s2x1*s2x2)
r12 <- 0
if(method_df == "perc_scale")   {se1adj <- sqrt((s2x1)*(1-r12)/n_samp)}   else   {se1adj <- sqrt((s2x1)*(1-r12)/(1/(n_samp-2)))/n_samp}
#     se2adj <- sqrt((s2x2)*(1-r12)/n_samp)}
#    se2adj <- sqrt((s2x2)*(1-r12)/(1/(n_samp-3)))/n_samp}
boot_low[i] <- rqr_beta1-t_adj*beta1_std
boot_high[i] <- rqr_beta1+t_adj*beta1_std
rank_low[i] <- beta1_rlb
rank_high[i] <- beta1_rub
evd_low[i] <- rqr_beta1+bk_pt[1]/se1adj
evd_high[i] <- rqr_beta1+bk_pt[2]/se1adj
evd_ci <- max(abs(bk_pt[1]),abs(bk_pt[2]))
evd_symm_low[i] <- rqr_beta1-evd_ci/se1adj
evd_symm_high[i] <- rqr_beta1+evd_ci/se1adj
bin_ci <- max(abs(bk_pt[3]),abs(bk_pt[4]))
bin_low[i] <- rqr_beta1-bin_ci/se1adj
bin_high[i] <- rqr_beta1+bin_ci/se1adj
#     boot_low2 <- rqr_beta2-t_adj*beta2_std
#     boot_high2 <- rqr_beta2+t_adj*beta2_std
#     evd_low2 <- rqr_beta2+bk_pt[1]/se2adj
#     evd_high2 <- rqr_beta2+bk_pt[2]/se2adj
#     evd_symm_low2 <- rqr_beta2-evd_ci/se2adj
#     evd_symm_high2 <- rqr_beta2+evd_ci/se2adj
#     bin_low2 <- rqr_beta2-bin_ci/se2adj
#     bin_high2 <- rqr_beta2+bin_ci/se2adj
samp_low <- min(samp)
samp_high <- max(samp)
pt_est <- qr_dat
pt_est_b1[i] <- rqr_beta1
pt_est_b0[i] <- rqr_beta0
if(method_df == "perc_scale")   {denxvar2 <- sqrt(1+(med_pt_est_x)^2/(se1adj)^2)}     else   {denxvar2 <- sqrt(n_samp/(n_samp-2)+(med_pt_est_x)^2/(se1adj)^2)}
# +
#                         (med_pt_est_x2)^2/(se2adj)^2)}
#     else   {denxvar2 <- sqrt(n_samp/(n_samp-2)+
#                                (med_pt_est_x)^2/(se1adj)^2)}
# +
#                         (med_pt_est_x2)^2/(se2adj)^2)}
denxevd2 <- denxvar2
denxbin2 <- denxvar2
boot_y0_low[i] <- rqr_beta0-t_adj*beta0_std
boot_y0_high[i] <- rqr_beta0+t_adj*beta0_std
rank_y0_low[i] <- beta0_rlb
rank_y0_high[i] <- beta0_rub
evd_y0_low[i] <- rqr_beta0+bk_pt[1]*denxevd2
evd_y0_high[i] <- rqr_beta0+bk_pt[2]*denxevd2
evd_symm_y0_low[i] <- rqr_beta0-evd_ci*denxevd2
evd_symm_y0_high[i] <- rqr_beta0+evd_ci*denxevd2
bin_y0_low[i] <- rqr_beta0-bin_ci*denxbin2
bin_y0_high[i] <- rqr_beta0+bin_ci*denxbin2
if(method_df == "perc_scale") {samp_var_theta <- (bin_ci/t_adj)^2} else {samp_var_theta <- (bin_ci/t_adj)^2*n_samp/(n_samp-2)}
mea_lm <- summary(lm(y_sample~x_sample))$r.squared
rsq <- 1- samp_var/y_var
samp_r1
# print(paste(boot_low[i],boot_high[i]))
# print(paste(rank_low[i],rank_high[i]))
# print(paste(evd_low[i],evd_high[i] ))
# print(paste(evd_symm_low[i],evd_symm_high[i]))
# print(paste(bin_low[i],bin_high[i]))
#
# pt_est
# print(paste(pt_est_b1[i],pt_est_b0[i]))
#
# med_pt_est_x
#
#
# print(paste(boot_y0_low[i],boot_y0_high[i]))
# print(paste(rank_y0_low[i],rank_y0_high[i]))
# print(paste(evd_y0_low[i],evd_y0_high[i]))
# print(paste(evd_symm_y0_low[i],evd_symm_y0_high[i]))
# print(paste(bin_y0_low[i],bin_y0_high[i]))
}
list(as.numeric(c(taus_set, pt_est_b0,
rank_y0_low,rank_y0_high,boot_y0_low,boot_y0_high,
evd_symm_y0_low,evd_symm_y0_high,bin_y0_low,bin_y0_high,
pt_est_b1,
rank_low,rank_high,boot_low,boot_high,
evd_symm_low,evd_symm_high,bin_low,bin_high)))
}
```
```{r, Engel_quasi-homoscedastic, echo=FALSE, cache=FALSE, fig.width=6, fig.height=6, fig.keep='high',warning=FALSE}
CI_results <- CI_estimator(log(engel$foodexp),log(engel$income),"perc_scale")
taus_set <- CI_results[[1]][1:50]
pt_est_b0 <- CI_results[[1]][51:100]
rank_y0_low <- CI_results[[1]][101:150]
rank_y0_high <- CI_results[[1]][151:200]
boot_y0_low <- CI_results[[1]][201:250]
boot_y0_high <- CI_results[[1]][251:300]
evd_symm_y0_low <- CI_results[[1]][301:350]
evd_symm_y0_high <- CI_results[[1]][351:400]
bin_y0_low <- CI_results[[1]][401:450]
bin_y0_high <- CI_results[[1]][451:500]
pt_est_b1 <- CI_results[[1]][501:550]
rank_low <- CI_results[[1]][551:600]
rank_high <- CI_results[[1]][601:650]
boot_low <- CI_results[[1]][651:700]
boot_high <- CI_results[[1]][701:750]
evd_symm_low <- CI_results[[1]][751:800]
evd_symm_high <- CI_results[[1]][801:850]
bin_low <- CI_results[[1]][851:900]
bin_high <- CI_results[[1]][901:950]
par(mfrow=c(2,3))
par(fig=c(0,.333,0.435,.935))
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b0, ylim = c(-.2,1.2), pch=20,
xlab="",ylab="",main=paste("rank inversion","\n intercept"))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(rank_y0_low,rev(rank_y0_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b0,  pch=20)
lines(x=taus_set, y=boot_y0_low, col="red",lty=2)
lines(x=taus_set, y=boot_y0_high, col="red",lty=2)
par(fig=c(0,.333,0,0.5), new=TRUE)
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b1, ylim = c(.6,1),
xlab="",main="income",mar=c(1,1,1,1))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(rank_low,rev(rank_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b1, pch=20)
lines(x=taus_set, y=boot_low, col="red",lty=2)
lines(x=taus_set, y=boot_high, col="red",lty=2)
par(fig=c(0.333,.666,0.435,.935),new=TRUE)
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b0, ylim = c(-.2,1.2), pch=20,
xlab="",ylab="",main=paste("default bootstrap","\n intercept"))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(boot_y0_low,rev(boot_y0_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b0,  pch=20)
par(fig=c(0.333,.666,0,0.5), new=TRUE)
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b1, ylim = c(.6,1),
xlab="",main="income",mar=c(1,1,1,1))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(boot_low,rev(boot_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b1, pch=20)
par(fig=c(0.666,1,0.435,.935),new=TRUE)
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b0, ylim = c(-.2,1.2), pch=20,
xlab="",ylab="",main=paste("evd symm max","\n intercept"))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(evd_symm_y0_low,rev(evd_symm_y0_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b0,  pch=20)
#add red lines on borders of polygon
lines(x=taus_set, y=boot_y0_low, col="red",lty=2)
lines(x=taus_set, y=boot_y0_high, col="red",lty=2)
par(fig=c(0.666,1,0,0.5), new=TRUE)
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b1, ylim = c(.6,1),
xlab="",main="income",mar=c(1,1,1,1))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(evd_symm_low,rev(evd_symm_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b1, pch=20)
#add red lines on borders of polygon
lines(x=taus_set, y=boot_low, col="red",lty=2)
lines(x=taus_set, y=boot_high, col="red",lty=2)
mtext("ENGELS log10(foodexp) v log10(income) quantile regression", side=3, outer=TRUE, line=-1.5)
mtext("approximately homoscedastic iid conditions", side=3, outer=TRUE, line=-3)
```
***Figure A2: Estimated intercept and slope model coefficient CIs for log-log transformed Engel dataset***
For completeness, figure A3 gives the comparative performance of the three CI estimators for the original heteroscedastic scattered dataset. It can be seen, the delete-d-group jackknife CI estimates are larger than both the evd_symm_max and rank inversion CI estimates. So the evd based approach will need further development for non-iid cases.
```{r, engel_heteroscedastic, echo=FALSE, cache=FALSE, fig.width=6, fig.height=6, fig.keep='high'}
CI_results <- CI_estimator(engel$foodexp,engel$income,"perc_scale")
taus_set <- CI_results[[1]][1:50]
pt_est_b0 <- CI_results[[1]][51:100]
rank_y0_low <- CI_results[[1]][101:150]
rank_y0_high <- CI_results[[1]][151:200]
boot_y0_low <- CI_results[[1]][201:250]
boot_y0_high <- CI_results[[1]][251:300]
evd_symm_y0_low <- CI_results[[1]][301:350]
evd_symm_y0_high <- CI_results[[1]][351:400]
bin_y0_low <- CI_results[[1]][401:450]
bin_y0_high <- CI_results[[1]][451:500]
pt_est_b1 <- CI_results[[1]][501:550]
rank_low <- CI_results[[1]][551:600]
rank_high <- CI_results[[1]][601:650]
boot_low <- CI_results[[1]][651:700]
boot_high <- CI_results[[1]][701:750]
evd_symm_low <- CI_results[[1]][751:800]
evd_symm_high <- CI_results[[1]][801:850]
bin_low <- CI_results[[1]][851:900]
bin_high <- CI_results[[1]][901:950]
par(mfrow=c(2,3))
par(fig=c(0,.333,0.435,.935))
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b0, ylim = c(0,200), pch=20,
xlab="",ylab="",main=paste("rank inversion","\n intercept"))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(rank_y0_low,rev(rank_y0_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b0,  pch=20)
lines(x=taus_set, y=boot_y0_low, col="red",lty=2)
lines(x=taus_set, y=boot_y0_high, col="red",lty=2)
par(fig=c(0,.333,0,0.5), new=TRUE)
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b1, ylim = c(.3,0.9),
xlab="",main="log10(income)",mar=c(1,1,1,1))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(rank_low,rev(rank_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b1, pch=20)
lines(x=taus_set, y=boot_low, col="red",lty=2)
lines(x=taus_set, y=boot_high, col="red",lty=2)
par(fig=c(0.333,.666,0.435,.935),new=TRUE)
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b0, ylim = c(0,200), pch=20,
xlab="",ylab="",main=paste("default bootstrap","\n intercept"))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(boot_y0_low,rev(boot_y0_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b0,  pch=20)
par(fig=c(0.333,.666,0,0.5), new=TRUE)
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b1, ylim = c(.3,0.9),
xlab="",main="log10(income)",mar=c(1,1,1,1))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(boot_low,rev(boot_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b1, pch=20)
par(fig=c(0.666,1,0.435,.935),new=TRUE)
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b0, ylim = c(0,200), pch=20,
xlab="",ylab="",main=paste("evd symm max","\n intercept"))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(evd_symm_y0_low,rev(evd_symm_y0_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b0,  pch=20)
#add red lines on borders of polygon
lines(x=taus_set, y=boot_y0_low, col="red",lty=2)
lines(x=taus_set, y=boot_y0_high, col="red",lty=2)
par(fig=c(0.666,1,0,0.5), new=TRUE)
par(mar=c(3,3,3,1.5)+0.1)
plot(x=taus_set, y=pt_est_b1, ylim = c(.3,0.9),
xlab="",main="log10(income)",mar=c(1,1,1,1))
#make polygon where coordinates start with lower limit and
# then upper limit in reverse order
polygon(c(taus_set,rev(taus_set)),c(evd_symm_low,rev(evd_symm_high)),col = "grey75", border = FALSE)
points(x=taus_set, y=pt_est_b1, pch=20)
#add red lines on borders of polygon
lines(x=taus_set, y=boot_low, col="red",lty=2)
lines(x=taus_set, y=boot_high, col="red",lty=2)
mtext("ENGELS foodexp v income quantile regression", side=3, outer=TRUE, line=-1.5)
mtext("heteroscedastic error behaviour", side=3, outer=TRUE, line=-3)
```
***Figure A3: Estimated model coefficient CIs for non-transformed Engel dataset***
setwd("C:/Users/John/Documents/GitHub/sampling-investigations/modelCIheterogeneouscaseresults")
getwd
min(linear1000[,4],linear1000[,5])
reg1_50perc <- read.csv(file="../modelCIheterogeneouscaseresults/50_runif_intleft_p15_dfadj_in_perc_scale_aux_reg.csv")
reg1_100perc <- read.csv(file="../modelCIheterogeneouscaseresults/100_runif_intleft_p15_dfadj_in_perc_scale_aux_reg.csv")
reg1_1000perc <- read.csv(file="../modelCIheterogeneouscaseresults/1000_runif_intleft_p15_dfadj_in_perc_scale_aux_reg.csv")
